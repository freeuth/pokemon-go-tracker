from fastapi import APIRouter
from pydantic import BaseModel
from typing import List, Dict
import logging
from app.services.pokedex_data_loader import get_data_loader
from app.services.crawler_service import crawler
from app.services.email_service import email_service
from app.core.database import SessionLocal
from app.models.event import Event

router = APIRouter(prefix="/api/admin", tags=["Admin"])
logger = logging.getLogger(__name__)


class ReloadResponse(BaseModel):
    status: str
    message: str


@router.post("/reload-data", response_model=ReloadResponse)
async def reload_data():
    """
    Reload all JSON data files without restarting the server

    Use this endpoint after updating any of the data files:
    - pokemon_base.json
    - moves.json
    - pokemon_moves.json
    - seasonal_tiers.json
    - raid_counters.json
    - pvp_party_rankings.json

    This allows you to update season data, add new PokÃ©mon, or modify
    tier rankings without server downtime.
    """
    try:
        loader = get_data_loader()
        loader.reload_data()

        return ReloadResponse(
            status="success",
            message="All data files reloaded successfully"
        )
    except Exception as e:
        return ReloadResponse(
            status="error",
            message=f"Failed to reload data: {str(e)}"
        )


@router.get("/data-stats")
async def get_data_stats():
    """
    Get statistics about loaded data

    Returns counts of:
    - PokÃ©mon
    - Moves
    - Seasonal tiers
    - Raid counters
    - PvP rankings
    """
    loader = get_data_loader()

    return {
        "pokemon_count": len(loader.pokemon_base),
        "moves_count": len(loader.moves),
        "pokemon_moves_count": len(loader.pokemon_moves),
        "seasonal_tiers_count": len(loader.seasonal_tiers),
        "raid_counters_count": len(loader.raid_counters),
        "pvp_rankings_count": len(loader.pvp_party_rankings),
        "current_season": loader.get_current_season()
    }


class CrawlNowResponse(BaseModel):
    status: str
    message: str
    new_events_count: int
    new_events: List[Dict]
    email_sent: bool


@router.post("/crawl-now", response_model=CrawlNowResponse)
async def manual_crawl_now():
    """
    Manual trigger for event crawling and email notification

    ìˆ˜ë™ìœ¼ë¡œ ì´ë²¤íŠ¸ í¬ë¡¤ë§ + ì´ë©”ì¼ ë°œì†¡ì„ ì¦‰ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ìŠ¤ì¼€ì¤„ê³¼ ë¬´ê´€í•˜ê²Œ ì¦‰ì‹œ ì‹¤í–‰ë˜ë©°, í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ë™ì‘:
    1. í¬ì¼“ëª¬GO ê³µì‹ í•œêµ­ì–´ í˜ì´ì§€ í¬ë¡¤ë§
    2. ì‹ ê·œ ì´ë²¤íŠ¸ í™•ì¸ (DBì™€ ë¹„êµ)
    3. ì‹ ê·œ ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ treehi1@gmail.comìœ¼ë¡œ ì´ë©”ì¼ ë°œì†¡

    Returns:
    - status: "success" ë˜ëŠ” "error"
    - message: ê²°ê³¼ ë©”ì‹œì§€
    - new_events_count: ì‹ ê·œ ì´ë²¤íŠ¸ ê°œìˆ˜
    - new_events: ì‹ ê·œ ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    - email_sent: ì´ë©”ì¼ ë°œì†¡ ì—¬ë¶€
    """
    logger.info("ğŸ”§ Manual crawl triggered via /api/admin/crawl-now")

    db = SessionLocal()
    new_events = []
    email_sent = False

    try:
        # 1. ì´ë²¤íŠ¸ í¬ë¡¤ë§
        logger.info("ğŸ“¡ Crawling Pokemon GO official Korean news page...")
        events = await crawler.fetch_events()
        logger.info(f"âœ… Found {len(events)} total events from website")

        # 2. ì‹ ê·œ ì´ë²¤íŠ¸ í™•ì¸ ë° DB ì €ì¥
        for event_data in events:
            existing_event = db.query(Event).filter(Event.url == event_data['url']).first()

            if not existing_event:
                # ì‹ ê·œ ì´ë²¤íŠ¸ ë°œê²¬
                new_event = Event(
                    title=event_data['title'],
                    url=event_data['url'],
                    summary=event_data.get('summary'),
                    published_date=event_data.get('published_date'),
                    image_url=event_data.get('image_url'),
                    category=event_data.get('category', 'ë‰´ìŠ¤')
                )
                db.add(new_event)
                new_events.append(event_data)
                logger.info(f"ğŸ†• New event found: {event_data['title']}")

        db.commit()

        # 3. ì´ë©”ì¼ ë°œì†¡ (ì‹ ê·œ ì´ë²¤íŠ¸ê°€ ìˆì„ ë•Œë§Œ)
        if new_events:
            logger.info(f"ğŸ“§ Sending email for {len(new_events)} new events...")
            email_sent = email_service.send_daily_news_summary(new_events, None)

            if email_sent:
                logger.info("âœ… Email sent successfully")
            else:
                logger.warning("âš ï¸ Email sending failed")
        else:
            logger.info("ğŸ“­ No new events to email")

        return CrawlNowResponse(
            status="success",
            message=f"Crawled {len(events)} events. Found {len(new_events)} new events.",
            new_events_count=len(new_events),
            new_events=new_events,
            email_sent=email_sent
        )

    except Exception as e:
        db.rollback()
        logger.error(f"âŒ Manual crawl failed: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")

        return CrawlNowResponse(
            status="error",
            message=f"Crawl failed: {str(e)}",
            new_events_count=0,
            new_events=[],
            email_sent=False
        )

    finally:
        db.close()


@router.get("/scheduler-status")
async def get_scheduler_status():
    """
    ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸ API

    ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    from app.scheduler import get_scheduler_status

    try:
        status = get_scheduler_status()
        return {
            "status": "success",
            "scheduler": status
        }
    except Exception as e:
        logger.error(f"âŒ Failed to get scheduler status: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }
