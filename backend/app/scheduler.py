from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
from typing import Dict
from datetime import datetime
from app.core.config import settings
from app.core.database import SessionLocal
from app.services.crawler_service import crawler
from app.services.email_service import email_service
from app.services.youtube_service import youtube_service
from app.models.event import Event
from app.models.youtube_video import YouTubeVideo
from app.models.email_subscription import EmailSubscription
import logging
import traceback

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Render í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ ì„¤ì •
scheduler = AsyncIOScheduler(
    timezone='Asia/Seoul',
    job_defaults={
        'coalesce': True,  # ëˆ„ë½ëœ ì‹¤í–‰ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
        'max_instances': 1,  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
        'misfire_grace_time': 300  # 5ë¶„ ì§€ì—°ê¹Œì§€ í—ˆìš©
    }
)


async def scheduled_crawl_job():
    """
    Scheduled job to crawl Pokemon GO events
    ë§¤ì¼ 10:00 (Asia/Seoul) ì‹¤í–‰ - ìƒˆ ë‰´ìŠ¤ê°€ ìˆì„ ë•Œë§Œ ì´ë©”ì¼ ë°œì†¡
    Render í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ìë™ ì‹¤í–‰
    """
    logger.info("=" * 80)
    logger.info(f"ğŸ• [SCHEDULER] Scheduled crawl job started at {datetime.now()}")
    logger.info("=" * 80)

    db = SessionLocal()
    new_events = []

    try:
        # 1. í¬ë¡¤ë§
        logger.info("ğŸ“¡ Fetching events from Pokemon GO website...")
        events = await crawler.fetch_events()
        logger.info(f"âœ… Found {len(events)} total events from website")

        # 2. ì‹ ê·œ ì´ë²¤íŠ¸ í™•ì¸ ë° ì €ì¥
        for event_data in events:
            existing_event = db.query(Event).filter(Event.url == event_data['url']).first()

            if not existing_event:
                new_event = Event(
                    title=event_data['title'],
                    url=event_data['url'],
                    summary=event_data.get('summary'),
                    published_date=event_data.get('published_date'),
                    image_url=event_data.get('image_url'),
                    category=event_data.get('category'),
                    is_notified=False
                )
                db.add(new_event)
                new_events.append(event_data)
                logger.info(f"ğŸ†• New event: {event_data['title'][:50]}...")

        db.commit()

        # 3. ì´ë©”ì¼ ë°œì†¡ (ì‹ ê·œ ì´ë²¤íŠ¸ê°€ ìˆì„ ë•Œë§Œ)
        if new_events:
            logger.info(f"ğŸ“§ {len(new_events)} new events found. Sending email...")
            email_sent = email_service.send_daily_news_summary(new_events, None)

            if email_sent:
                # ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ ì‹œ is_notified ì—…ë°ì´íŠ¸
                for event_data in new_events:
                    event = db.query(Event).filter(Event.url == event_data['url']).first()
                    if event:
                        event.is_notified = True
                db.commit()
                logger.info(f"âœ… Email sent successfully for {len(new_events)} events")
            else:
                logger.error("âŒ Email sending failed")
        else:
            logger.info("ğŸ“­ No new events. No email sent.")

        logger.info(f"âœ… Scheduled crawl completed successfully")

    except Exception as e:
        logger.error(f"âŒ ERROR in scheduled_crawl_job:")
        logger.error(f"   Error type: {type(e).__name__}")
        logger.error(f"   Error message: {str(e)}")
        logger.error(f"   Traceback:\n{traceback.format_exc()}")
        db.rollback()

    finally:
        db.close()
        logger.info("=" * 80)


async def scheduled_youtube_crawl_job():
    """
    Scheduled job to crawl YouTube battle videos from RSS feeds
    ë§¤ì¼ ì˜¤ì „ 10ì‹œ ì‹¤í–‰ (Asia/Seoul)
    - ìµœê·¼ 2ì£¼ì¼ ì˜ìƒë§Œ ìˆ˜ì§‘
    - 3ê°œì›” ì§€ë‚œ ì˜ìƒ ìë™ ì‚­ì œ
    """
    logger.info("Starting scheduled YouTube RSS feed crawl...")
    db = SessionLocal()
    new_videos = []  # ìƒˆë¡œ ì¶”ê°€ëœ ì˜ìƒ ëª©ë¡

    try:
        # 1. 3ê°œì›” ì§€ë‚œ ì˜ìƒ ì‚­ì œ
        from datetime import datetime, timedelta
        three_months_ago = datetime.now() - timedelta(days=90)

        deleted_count = db.query(YouTubeVideo).filter(
            YouTubeVideo.published_at < three_months_ago
        ).delete()

        if deleted_count > 0:
            logger.info(f"Deleted {deleted_count} videos older than 3 months")

        # 2. ìµœê·¼ 2ì£¼ì¼ ì˜ìƒ ìˆ˜ì§‘ (fetch_latest_videosì—ì„œ í•„í„°ë§ë¨)
        videos = await youtube_service.fetch_latest_videos(max_results=50)
        new_videos_count = 0

        for video_data in videos:
            # Check if video already exists
            existing_video = db.query(YouTubeVideo).filter(
                YouTubeVideo.video_id == video_data['video_id']
            ).first()

            if not existing_video:
                # Create new video entry
                new_video = YouTubeVideo(
                    video_id=video_data['video_id'],
                    title=video_data['title'],
                    channel_name=video_data['channel_name'],
                    channel_id=video_data.get('channel_id'),
                    thumbnail_url=video_data.get('thumbnail_url'),
                    description=video_data.get('description'),
                    published_at=video_data.get('published_at'),
                    video_url=video_data.get('video_url'),
                    view_count=video_data.get('view_count', 0),
                    tags=video_data.get('tags', [])
                )
                db.add(new_video)
                new_videos.append(video_data)
                new_videos_count += 1

        db.commit()
        logger.info(f"YouTube RSS crawl completed. Found {new_videos_count} new videos from RSS feeds.")

    except Exception as e:
        logger.error(f"Error during scheduled YouTube RSS crawl: {str(e)}")
        db.rollback()
    finally:
        db.close()


def start_scheduler():
    """
    Initialize and start the scheduler
    - ë‰´ìŠ¤ í¬ë¡¤ë§: ë§¤ì¼ 10:00 (Asia/Seoul)
    - YouTube ì˜ìƒ í¬ë¡¤ë§: ë§¤ì¼ 10:00 (Asia/Seoul) - ìµœê·¼ 2ì£¼ì¼ ì˜ìƒë§Œ, 3ê°œì›” ì§€ë‚œ ì˜ìƒ ìë™ ì‚­ì œ

    Render í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    ì´ˆê¸° í¬ë¡¤ë§ì€ ìˆ˜í–‰í•˜ì§€ ì•Šìœ¼ë©°, ìŠ¤ì¼€ì¤„ì— ë”°ë¼ì„œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    # Schedule the event crawl job to run daily at 10:00 AM (Asia/Seoul)
    scheduler.add_job(
        scheduled_crawl_job,
        trigger=CronTrigger(hour=10, minute=0, timezone='Asia/Seoul'),
        id='pokemon_go_crawler',
        name='Crawl Pokemon GO events (Daily 10:00 Asia/Seoul)',
        replace_existing=True
    )

    # Schedule the YouTube video crawl job to run daily at 10:00 AM (Asia/Seoul)
    scheduler.add_job(
        scheduled_youtube_crawl_job,
        trigger=CronTrigger(hour=10, minute=0, timezone='Asia/Seoul'),
        id='youtube_video_crawler',
        name='Crawl YouTube battle videos (Daily 10:00 Asia/Seoul)',
        replace_existing=True
    )

    scheduler.start()
    logger.info(f"âœ… Scheduler started successfully")
    logger.info(f"ğŸ“… Pokemon GO news: Daily at 10:00 AM (Asia/Seoul)")
    logger.info(f"ğŸ¬ YouTube videos: Daily at 10:00 AM (Asia/Seoul)")
    logger.info(f"ğŸ’¡ Use POST /api/admin/crawl-now for manual testing")


def stop_scheduler():
    """Stop the scheduler"""
    scheduler.shutdown()
    logger.info("Scheduler stopped.")


def get_scheduler_status() -> Dict:
    """
    ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸
    Render í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì •ìƒ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ëŠ” APIìš©
    """
    try:
        is_running = scheduler.running
        jobs = scheduler.get_jobs()

        jobs_info = []
        for job in jobs:
            next_run = job.next_run_time
            jobs_info.append({
                "id": job.id,
                "name": job.name,
                "next_run_time": next_run.isoformat() if next_run else None,
                "trigger": str(job.trigger)
            })

        return {
            "running": is_running,
            "timezone": str(scheduler.timezone),
            "jobs_count": len(jobs),
            "jobs": jobs_info,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ Error getting scheduler status: {str(e)}")
        return {
            "running": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
